{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d13d9625",
   "metadata": {},
   "source": [
    "The team for this project is:\n",
    "- Marc-César Garcia-Grenet\n",
    "- Thomas Bodart\n",
    "- Macéo Duriez\n",
    "\n",
    "This notebook contains half of the work done for the task 4, which consists of a deeper exploration of the task 1. This work was done by Thomas Bodart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e862ae3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import highway_env\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from copy import deepcopy\n",
    "from collections import deque\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88711282",
   "metadata": {},
   "source": [
    "Some utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59e3fa3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "def run_one_episode(env, agent, display=True):\n",
    "    display_env = deepcopy(env)\n",
    "    done = False\n",
    "    state, _ = display_env.reset()\n",
    "\n",
    "    rewards = 0\n",
    "\n",
    "    while not done:\n",
    "        action = agent.get_action(state, use_epsilon=False)\n",
    "        state, reward, terminated, truncated, _ = display_env.step(action)\n",
    "        done = terminated or truncated\n",
    "        rewards += reward\n",
    "        if display: \n",
    "            clear_output(wait=True)\n",
    "            plt.imshow(display_env.render())\n",
    "            plt.show()\n",
    "    if display:\n",
    "        display_env.close()\n",
    "    print(f'total episode reward {rewards}')\n",
    "\n",
    "def eval_agent(agent, env, n_sim=10):\n",
    "    \"\"\"    \n",
    "    Monte Carlo evaluation of the agent.\n",
    "\n",
    "    Repeat n_sim times:\n",
    "        * Run the agent policy until the environment reaches a terminal state (= one episode)\n",
    "        * Compute the sum of rewards in this episode\n",
    "        * Store the sum of rewards in the episode_rewards array.\n",
    "    \"\"\"\n",
    "    env_copy = deepcopy(env)\n",
    "    episode_rewards = np.zeros(n_sim)\n",
    "    for i in range(n_sim):\n",
    "        state, _ = env_copy.reset()\n",
    "        reward_sum = 0\n",
    "        done = False\n",
    "        while not done: \n",
    "            action = agent.get_action(state, use_epsilon=False)\n",
    "            state, reward, terminated, truncated, _ = env_copy.step(action)\n",
    "            reward_sum += reward\n",
    "            done = terminated or truncated\n",
    "        episode_rewards[i] = reward_sum\n",
    "    return episode_rewards\n",
    "\n",
    "class RandomAgent: \n",
    "    def __init__(self, observation_space, action_space):\n",
    "        self.action_space = action_space\n",
    "        return\n",
    "    \n",
    "    def get_action(self, state, **kwargs):\n",
    "        return self.action_space.sample()\n",
    "    \n",
    "    def update(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57deec9f",
   "metadata": {},
   "source": [
    "More utilities: training, training curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "182171ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, agent, N_episodes, eval_every=100, reward_threshold=300, n_eval=10, save_model=False):\n",
    "    total_time = 0\n",
    "    all_rewards = []\n",
    "    all_lengths = []\n",
    "    eval_rewards = []\n",
    "    \n",
    "    for ep in tqdm(range(N_episodes), desc=\"Training...\"):\n",
    "        done = False\n",
    "        state, _ = env.reset()\n",
    "        ep_reward = 0\n",
    "        steps = 0\n",
    "\n",
    "        while not done: \n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            \n",
    "            # agent.store_transition(state, action, actual_reward, next_state, terminated)\n",
    "            agent.store_transition(state, action, reward, next_state, terminated)\n",
    "            agent.update()\n",
    "\n",
    "            state = next_state\n",
    "            done = terminated or truncated\n",
    "\n",
    "            ep_reward += reward\n",
    "            steps += 1\n",
    "            total_time += 1\n",
    "\n",
    "        all_rewards.append(ep_reward)\n",
    "        all_lengths.append(steps)\n",
    "\n",
    "        if (ep + 1) % eval_every == 0:\n",
    "            mean_eval_reward = np.mean(eval_agent(agent, env, n_sim=n_eval))\n",
    "            eval_rewards.append((ep + 1, mean_eval_reward))\n",
    "            print(f\"[Episode {ep+1}] Eval reward: {mean_eval_reward:.2f} | Train reward: {ep_reward:.2f} | Steps: {steps}\")\n",
    "            \n",
    "            if save_model:\n",
    "                torch.save(agent.policy_net.state_dict(), f\"model_ep{ep+1}.pt\")\n",
    "\n",
    "            if mean_eval_reward >= reward_threshold:\n",
    "                print(\"Reward threshold reached! Stopping early.\")\n",
    "                break\n",
    "\n",
    "    print(\"\\n Training complete.\")\n",
    "    _plot_training_curves(all_rewards, eval_rewards, agent.learning_rate)\n",
    "    return all_rewards, eval_rewards, all_lengths\n",
    "\n",
    "\n",
    "def _plot_training_curves(all_rewards, eval_rewards, learning_rate):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Smoothed training reward\n",
    "    smoothed_rewards = np.convolve(all_rewards, np.ones(10) / 10, mode='valid')\n",
    "    plt.plot(smoothed_rewards, label='Train Reward (smoothed)', alpha=0.7)\n",
    "\n",
    "    if eval_rewards:\n",
    "        eval_eps, eval_vals = zip(*eval_rewards)\n",
    "        plt.plot(eval_eps, eval_vals, 'o-', label='Eval Reward', color='red')\n",
    "\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.title(f'Training and Evaluation Reward Over Time, with lr: {learning_rate}')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea89f41",
   "metadata": {},
   "source": [
    "## New environment\n",
    "No more reward for being on the rightmost lane, and more reward for being fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "858e2210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total episode reward 3.8084450831043317\n"
     ]
    }
   ],
   "source": [
    "\n",
    "config_dict = {\n",
    "    \"observation\": {\n",
    "        \"type\": \"OccupancyGrid\",\n",
    "        \"vehicles_count\": 10,\n",
    "        \"features\": [\"presence\", \"x\", \"y\", \"vx\", \"vy\", \"cos_h\", \"sin_h\"],\n",
    "        \"features_range\": {\n",
    "            \"x\": [-100, 100],\n",
    "            \"y\": [-100, 100],\n",
    "            \"vx\": [-20, 20],\n",
    "            \"vy\": [-20, 20],\n",
    "        },\n",
    "        \"grid_size\": [[-20, 20], [-20, 20]],\n",
    "        \"grid_step\": [5, 5],\n",
    "        \"absolute\": False,\n",
    "    },\n",
    "    \"action\": {\n",
    "        \"type\": \"DiscreteMetaAction\",\n",
    "    },\n",
    "    \"lanes_count\": 4,\n",
    "    \"vehicles_count\": 15,\n",
    "    \"duration\": 80,  # [s]\n",
    "    \"initial_spacing\": 0,\n",
    "    \"collision_reward\": -1,  # The reward received when colliding with a vehicle.\n",
    "    \"right_lane_reward\": 0,  # The reward received when driving on the right-most lanes, linearly mapped to\n",
    "    # zero for other lanes.\n",
    "    \"high_speed_reward\": 0.5,  # The reward received when driving at full speed, linearly mapped to zero for\n",
    "    # lower speeds according to config[\"reward_speed_range\"].\n",
    "    \"reward_speed_range\": [\n",
    "        20,\n",
    "        30,\n",
    "    ],  # [m/s] The reward for high speed is mapped linearly from this range to [0, HighwayEnv.HIGH_SPEED_REWARD].\n",
    "    \"simulation_frequency\": 5,  # [Hz]\n",
    "    \"policy_frequency\": 1,  # [Hz]\n",
    "    \"other_vehicles_type\": \"highway_env.vehicle.behavior.IDMVehicle\",\n",
    "    \"screen_width\": 600,  # [px]\n",
    "    \"screen_height\": 150,  # [px]\n",
    "    \"centering_position\": [0.3, 0.5],\n",
    "    \"scaling\": 5.5,\n",
    "    \"show_trajectories\": True,\n",
    "    \"render_agent\": True,\n",
    "    \"offscreen_rendering\": False,\n",
    "    \"disable_collision_checks\": True,\n",
    "}\n",
    "\n",
    "env = gym.make(\"highway-fast-v0\", render_mode=\"rgb_array\", config = config_dict)\n",
    "env.reset()\n",
    "\n",
    "\n",
    "run_one_episode(env, RandomAgent(env.observation_space, env.action_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b4d595",
   "metadata": {},
   "source": [
    "## Same Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d1d5d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c1/j3mjhbjn00x3fcz1qgkn682h0000gn/T/ipykernel_11142/1006458154.py:27: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:257.)\n",
      "  ex = torch.tensor([env.observation_space.sample()])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0634, -0.0722,  0.1034,  0.2483,  0.0103]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.conv1 = nn.Conv2d(in_channels=7, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.ln1 = nn.LayerNorm([16, 8, 8])\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.ln2 = nn.LayerNorm([32, 8, 8])\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 128)\n",
    "        self.fc2 = nn.Linear(128, 5)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.flatten = nn.Flatten()\n",
    "    \n",
    "    def forward(self, observation_space: torch.FloatTensor):\n",
    "        x = self.relu(self.ln1(self.conv1(observation_space)))\n",
    "        x = self.relu(self.ln2(self.conv2(x)))\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "ex = torch.tensor([env.observation_space.sample()])\n",
    "model = CNN()\n",
    "model(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2a20dc",
   "metadata": {},
   "source": [
    "## New agent: Double DQN with PER\n",
    "I added two techniques, supposed to improve the results of the agent.\n",
    "### Prioritized Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821bd59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self, capacity, alpha=0.6):\n",
    "        self.capacity = capacity\n",
    "        self.alpha = alpha\n",
    "        self.buffer = []\n",
    "        self.priorities = np.zeros((capacity,), dtype=np.float32)\n",
    "        self.pos = 0\n",
    "\n",
    "    def push(self, *transition):\n",
    "        max_priority = self.priorities.max() if self.buffer else 1.0\n",
    "\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(transition)\n",
    "        else:\n",
    "            self.buffer[self.pos] = transition\n",
    "\n",
    "        self.priorities[self.pos] = max_priority\n",
    "        self.pos = (self.pos + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size, beta=0.4):\n",
    "        if len(self.buffer) == self.capacity:\n",
    "            probs = self.priorities\n",
    "        else:\n",
    "            probs = self.priorities[:self.pos]\n",
    "\n",
    "        probs = probs ** self.alpha\n",
    "        probs /= probs.sum()\n",
    "\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probs)\n",
    "        samples = [self.buffer[i] for i in indices]\n",
    "\n",
    "        total = len(self.buffer)\n",
    "        weights = (total * probs[indices]) ** (-beta)\n",
    "        weights /= weights.max()\n",
    "        weights = np.array(weights, dtype=np.float32)\n",
    "\n",
    "        return samples, indices, weights\n",
    "\n",
    "    def update_priorities(self, indices, priorities):\n",
    "        for i, p in zip(indices, priorities):\n",
    "            self.priorities[i] = p\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4a44be",
   "metadata": {},
   "source": [
    "### Double DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cba8b62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PERDoubleDQNAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        action_space,\n",
    "        observation_space,\n",
    "        gamma,\n",
    "        batch_size,\n",
    "        learning_rate,\n",
    "        weight_decay,\n",
    "        model,\n",
    "        optimizer,\n",
    "        buffer_capacity\n",
    "    ):\n",
    "        self.action_space = action_space\n",
    "        self.observation_space = observation_space\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "        self.policy_net = model.to(self.device)\n",
    "        self.target_net = deepcopy(model).to(self.device)\n",
    "        self.target_net.eval()\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.optimizer = optimizer(self.policy_net.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        self.replay_buffer = PrioritizedReplayBuffer(capacity=buffer_capacity)\n",
    "\n",
    "        self.steps = 0\n",
    "        self.update_target_every = 100\n",
    "\n",
    "        self.action_counter = Counter()\n",
    "\n",
    "        self.epsilon_start = 0.1\n",
    "        self.epsilon_end = 0.05\n",
    "        self.epsilon_decay = 500\n",
    "        self.epsilon = self.epsilon_start\n",
    "\n",
    "    def update(self):\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        (transitions, indices, weights) = self.replay_buffer.sample(self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*transitions)\n",
    "\n",
    "        weights = torch.tensor(weights, device=self.device, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "\n",
    "        states = torch.tensor(states, device=self.device, dtype=torch.float32)\n",
    "        actions = torch.tensor(actions, device=self.device).unsqueeze(1)\n",
    "        rewards = torch.tensor(rewards, device=self.device, dtype=torch.float32).unsqueeze(1)\n",
    "        next_states = torch.tensor(next_states, device=self.device, dtype=torch.float32)\n",
    "        dones = torch.tensor(dones, device=self.device, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "        q_values = self.policy_net(states).gather(1, actions)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_actions = self.policy_net(next_states).argmax(1, keepdim=True)\n",
    "            next_q_values = self.target_net(next_states).gather(1, next_actions)\n",
    "            target_q_values = rewards + self.gamma * next_q_values * (1 - dones)\n",
    "\n",
    "        td_errors = (q_values - target_q_values).detach().squeeze().abs().cpu().numpy()\n",
    "        loss = (torch.nn.functional.smooth_l1_loss(q_values, target_q_values, reduction='none') * weights).mean()\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.replay_buffer.update_priorities(indices, td_errors + 1e-5)\n",
    "\n",
    "        self.steps += 1\n",
    "        self.epsilon = self.epsilon_end + (self.epsilon_start - self.epsilon_end) * np.exp(-1. * self.step / self.epsilon_decay)\n",
    "        if self.steps % self.update_target_every == 0:\n",
    "            self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        self.replay_buffer.push(state, action, reward, next_state, done)\n",
    "\n",
    "    def get_action(self, state, use_epsilon=True):\n",
    "\n",
    "        if use_epsilon and random.random() < self.epsilon:\n",
    "            action = self.action_space.sample()\n",
    "            self.action_counter[action] += 1\n",
    "            return action \n",
    "\n",
    "        state_tensor = torch.tensor(state, device=self.device, dtype=torch.float32).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.policy_net(state_tensor)\n",
    "        \n",
    "        action = q_values.argmax().item()\n",
    "        self.action_counter[action] += 1\n",
    "        return action\n",
    "        \n",
    "    def reset_action_counter(self):\n",
    "        self.action_counter = Counter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0c6ef6",
   "metadata": {},
   "source": [
    "### Training\n",
    "Some experiments were done on the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df76be2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward before training =  26.5015386715718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:   0%|          | 0/1000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'PrioritizedReplayBuffer' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mmean reward before training = \u001b[39m\u001b[33m\"\u001b[39m, np.mean(eval_agent(agent, env, \u001b[32m10\u001b[39m)))\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Run the training loop\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_episodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_every\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# Evaluate the final policy\u001b[39;00m\n\u001b[32m     32\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mmean reward after training = \u001b[39m\u001b[33m\"\u001b[39m, np.mean(eval_agent(agent, env, \u001b[32m10\u001b[39m)))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(env, agent, N_episodes, eval_every, reward_threshold, n_eval, save_model)\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# agent.store_transition(state, action, actual_reward, next_state, terminated)\u001b[39;00m\n\u001b[32m     18\u001b[39m agent.store_transition(state, action, reward, next_state, terminated)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m state = next_state\n\u001b[32m     22\u001b[39m done = terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 39\u001b[39m, in \u001b[36mPERDoubleDQNAgent.update\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mupdate\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m)\u001b[49m < \u001b[38;5;28mself\u001b[39m.batch_size:\n\u001b[32m     40\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m     42\u001b[39m     (transitions, indices, weights) = \u001b[38;5;28mself\u001b[39m.replay_buffer.sample(\u001b[38;5;28mself\u001b[39m.batch_size)\n",
      "\u001b[31mTypeError\u001b[39m: object of type 'PrioritizedReplayBuffer' has no len()"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"highway-fast-v0\", render_mode=\"rgb_array\", config = config_dict)\n",
    "env.reset()\n",
    "\n",
    "action_space = env.action_space\n",
    "observation_space = env.observation_space\n",
    "\n",
    "gamma = .99\n",
    "episode_batch_size = 64\n",
    "learning_rate = 1e-4\n",
    "weight_decay = 0\n",
    "model=CNN()\n",
    "optimizer = torch.optim.Adam\n",
    "N_episodes = 1000\n",
    "\n",
    "agent = PERDoubleDQNAgent(\n",
    "    action_space=action_space,\n",
    "    observation_space=observation_space,\n",
    "    gamma=gamma,\n",
    "    batch_size=episode_batch_size,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    buffer_capacity=10_000,\n",
    ")\n",
    "\n",
    "print(\"mean reward before training = \", np.mean(eval_agent(agent, env, 10)))\n",
    "# Run the training loop\n",
    "train(env, agent, N_episodes, eval_every=50,)\n",
    "\n",
    "# Evaluate the final policy\n",
    "print(\"mean reward after training = \", np.mean(eval_agent(agent, env, 10)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e1b06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "agent.reset_action_counter()\n",
    "\n",
    "run_one_episode(env, agent)\n",
    "\n",
    "print(agent.action_counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299f6076",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
