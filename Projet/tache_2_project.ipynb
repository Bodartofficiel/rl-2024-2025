{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c53a7b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import highway_env\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9a1fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    \"\"\"Experience replay buffer to store and sample experiences.\"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))\n",
    "        return (np.array(state), np.array(action), \n",
    "                np.array(reward, dtype=np.float32), \n",
    "                np.array(next_state), np.array(done, dtype=np.uint8))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    \"\"\"Actor network for determining the actions to take with Layer Normalization.\"\"\"\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=512, log_std_min=-20, log_std_max=2):\n",
    "        super(Actor, self).__init__()\n",
    "        self.log_std_min = log_std_min\n",
    "        self.log_std_max = log_std_max\n",
    "        \n",
    "        # Feature extraction layers with Layer Normalization\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.ln1 = nn.LayerNorm(hidden_dim)\n",
    "        \n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.ln2 = nn.LayerNorm(hidden_dim)\n",
    "        \n",
    "        # Mean and log_std output layers\n",
    "        self.mu = nn.Linear(hidden_dim, action_dim)\n",
    "        self.log_std = nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.ln1(self.fc1(state)))\n",
    "        x = F.relu(self.ln2(self.fc2(x)))\n",
    "        \n",
    "        mu = self.mu(x)\n",
    "        \n",
    "        # Constrain log_std within reasonable range\n",
    "        log_std = self.log_std(x)\n",
    "        log_std = torch.clamp(log_std, self.log_std_min, self.log_std_max)\n",
    "        \n",
    "        return mu, log_std\n",
    "    \n",
    "    def sample(self, state):\n",
    "        mu, log_std = self.forward(state)\n",
    "        std = log_std.exp()\n",
    "        \n",
    "        # Use reparameterization trick\n",
    "        normal = Normal(mu, std)\n",
    "        z = normal.rsample()\n",
    "        \n",
    "        # Squash using tanh to bound actions between -1 and 1\n",
    "        action = torch.tanh(z)\n",
    "        \n",
    "        # Calculate log probability, incorporating the Jacobian adjustment for tanh\n",
    "        log_prob = normal.log_prob(z) - torch.log(1 - action.pow(2) + 1e-6)\n",
    "        log_prob = log_prob.sum(1, keepdim=True)\n",
    "        \n",
    "        return action, log_prob\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    \"\"\"Critic network for estimating value functions with Layer Normalization.\"\"\"\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=256):\n",
    "        super(Critic, self).__init__()\n",
    "        \n",
    "        # Q1 architecture with Layer Normalization\n",
    "        self.fc1 = nn.Linear(state_dim + action_dim, hidden_dim)\n",
    "        self.ln1 = nn.LayerNorm(hidden_dim)\n",
    "        \n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.ln2 = nn.LayerNorm(hidden_dim)\n",
    "        \n",
    "        self.q1 = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "        # Q2 architecture (for twin Q-learning) with Layer Normalization\n",
    "        self.fc3 = nn.Linear(state_dim + action_dim, hidden_dim)\n",
    "        self.ln3 = nn.LayerNorm(hidden_dim)\n",
    "        \n",
    "        self.fc4 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.ln4 = nn.LayerNorm(hidden_dim)\n",
    "        \n",
    "        self.q2 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        sa = torch.cat([state, action], 1)\n",
    "        \n",
    "        # Q1 estimate\n",
    "        q1 = F.relu(self.ln1(self.fc1(sa)))\n",
    "        q1 = F.relu(self.ln2(self.fc2(q1)))\n",
    "        q1 = self.q1(q1)\n",
    "        \n",
    "        # Q2 estimate\n",
    "        q2 = F.relu(self.ln3(self.fc3(sa)))\n",
    "        q2 = F.relu(self.ln4(self.fc4(q2)))\n",
    "        q2 = self.q2(q2)\n",
    "        \n",
    "        return q1, q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "848483e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAC:\n",
    "    \"\"\"Soft Actor-Critic algorithm.\"\"\"\n",
    "    def __init__(self, state_dim, action_dim, action_space, hidden_dim=256, lr=3e-4, gamma=0.99, tau=0.005, alpha=0.2, auto_entropy_tuning=True):\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.alpha = alpha\n",
    "        self.auto_entropy_tuning = auto_entropy_tuning\n",
    "        self.action_space = action_space\n",
    "        \n",
    "        # Initialize networks\n",
    "        self.actor = Actor(state_dim, action_dim, hidden_dim).to(device)\n",
    "        self.critic = Critic(state_dim, action_dim, hidden_dim).to(device)\n",
    "        self.critic_target = Critic(state_dim, action_dim, hidden_dim).to(device)\n",
    "        \n",
    "        # Copy parameters from critic to target\n",
    "        for target_param, param in zip(self.critic_target.parameters(), self.critic.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "        \n",
    "        # Setup optimizers\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr)\n",
    "        \n",
    "        # Automatic entropy tuning\n",
    "        if auto_entropy_tuning:\n",
    "            self.target_entropy = -torch.prod(torch.Tensor(action_space.shape).to(device)).item()\n",
    "            self.log_alpha = torch.zeros(1, requires_grad=True, device=device)\n",
    "            self.alpha_optimizer = optim.Adam([self.log_alpha], lr=lr)\n",
    "    \n",
    "    def select_action(self, state, evaluate=False):\n",
    "        state = torch.FloatTensor(state).to(device)\n",
    "        state = state.view(1, -1)\n",
    "\n",
    "        if evaluate:\n",
    "            # Use mean action for evaluation (no exploration)\n",
    "            with torch.no_grad():\n",
    "                mu, _ = self.actor(state)\n",
    "                return torch.tanh(mu).cpu().numpy()[0]\n",
    "        else:\n",
    "            # Sample action for training\n",
    "            with torch.no_grad():\n",
    "                action, _ = self.actor.sample(state)\n",
    "                return action.cpu().numpy()[0]\n",
    "    \n",
    "    def train(self, memory, batch_size=256):\n",
    "        if len(memory) < batch_size:\n",
    "            return\n",
    "        \n",
    "        state_batch, action_batch, reward_batch, next_state_batch, done_batch = memory.sample(batch_size)\n",
    "        \n",
    "        # Convert to tensor\n",
    "        state_batch = torch.FloatTensor(state_batch).to(device)\n",
    "        action_batch = torch.FloatTensor(action_batch).to(device)\n",
    "        reward_batch = torch.FloatTensor(reward_batch).to(device).unsqueeze(1)\n",
    "        next_state_batch = torch.FloatTensor(next_state_batch).to(device)\n",
    "        done_batch = torch.FloatTensor(done_batch).to(device).unsqueeze(1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            next_state_batch = next_state_batch.view(next_state_batch.size(0), -1)\n",
    "            next_action, next_log_prob = self.actor.sample(next_state_batch)\n",
    "            next_q1, next_q2 = self.critic_target(next_state_batch, next_action)\n",
    "            next_q = torch.min(next_q1, next_q2) - self.alpha * next_log_prob\n",
    "            target_q = reward_batch + (1 - done_batch) * self.gamma * next_q\n",
    "        \n",
    "        # Current Q estimates\n",
    "        state_batch = state_batch.view(state_batch.size(0), -1)\n",
    "        current_q1, current_q2 = self.critic(state_batch, action_batch)\n",
    "        \n",
    "        # Critic loss\n",
    "        critic_loss = F.mse_loss(current_q1, target_q) + F.mse_loss(current_q2, target_q)\n",
    "        \n",
    "        # Update critic\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.critic.parameters(), max_norm=1.0)\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        # Actor loss\n",
    "        action, log_prob = self.actor.sample(state_batch)\n",
    "        q1, q2 = self.critic(state_batch, action)\n",
    "        min_q = torch.min(q1, q2)\n",
    "        actor_loss = (self.alpha * log_prob - min_q).mean()\n",
    "        \n",
    "        # Update actor\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.actor.parameters(), max_norm=1.0)\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        # Update alpha if needed\n",
    "        if self.auto_entropy_tuning:\n",
    "            alpha_loss = -(self.log_alpha * (log_prob + self.target_entropy).detach()).mean()\n",
    "            self.alpha_optimizer.zero_grad()\n",
    "            alpha_loss.backward()\n",
    "            self.alpha_optimizer.step()\n",
    "            self.alpha = self.log_alpha.exp()\n",
    "        \n",
    "        # Soft update of target networks\n",
    "        for target_param, param in zip(self.critic_target.parameters(), self.critic.parameters()):\n",
    "            target_param.data.copy_(target_param.data * (1.0 - self.tau) + param.data * self.tau)\n",
    "        \n",
    "        return critic_loss.item(), actor_loss.item()\n",
    "    \n",
    "    def save(self, directory):\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        \n",
    "        torch.save(self.actor.state_dict(), os.path.join(directory, \"actor.pth\"))\n",
    "        torch.save(self.critic.state_dict(), os.path.join(directory, \"critic.pth\"))\n",
    "        torch.save(self.critic_target.state_dict(), os.path.join(directory, \"critic_target.pth\"))\n",
    "        \n",
    "        if self.auto_entropy_tuning:\n",
    "            torch.save(self.log_alpha, os.path.join(directory, \"log_alpha.pth\"))\n",
    "    \n",
    "    def load(self, directory):\n",
    "        self.actor.load_state_dict(torch.load(os.path.join(directory, \"actor.pth\")))\n",
    "        self.critic.load_state_dict(torch.load(os.path.join(directory, \"critic.pth\")))\n",
    "        self.critic_target.load_state_dict(torch.load(os.path.join(directory, \"critic_target.pth\")))\n",
    "        \n",
    "        if self.auto_entropy_tuning and os.path.exists(os.path.join(directory, \"log_alpha.pth\")):\n",
    "            self.log_alpha = torch.load(os.path.join(directory, \"log_alpha.pth\"))\n",
    "            self.alpha = self.log_alpha.exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49220a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizedEnv(gym.Wrapper):\n",
    "    \"\"\"Environment wrapper for normalizing observations and rewards.\"\"\"\n",
    "    def __init__(self, env):\n",
    "        super(NormalizedEnv, self).__init__(env)\n",
    "        self.state_mean = np.zeros(self.observation_space.shape[0])\n",
    "        self.state_std = np.ones(self.observation_space.shape[0])\n",
    "        self.state_buffer = deque(maxlen=10000)\n",
    "        self.reward_scale = 1.0\n",
    "        \n",
    "    def normalize_state(self, state):\n",
    "        return (state - self.state_mean) / (self.state_std + 1e-8)\n",
    "    \n",
    "    def update_state_stats(self):\n",
    "        if len(self.state_buffer) > 0:\n",
    "            states = np.array(self.state_buffer)\n",
    "            self.state_mean = np.mean(states, axis=0)\n",
    "            self.state_std = np.std(states, axis=0) + 1e-8\n",
    "    \n",
    "    def step(self, action):\n",
    "        try:\n",
    "            action = np.array(action[0])\n",
    "        except:\n",
    "            action = np.array(action)\n",
    "        next_state, reward, done, truncated, info = self.env.step(action)\n",
    "        self.state_buffer.append(next_state)\n",
    "        \n",
    "        return self.normalize_state(next_state), reward * self.reward_scale, done, truncated, info\n",
    "    \n",
    "    def reset(self, **kwargs):\n",
    "        state, info = self.env.reset(**kwargs)\n",
    "        self.state_buffer.append(state)\n",
    "        self.update_state_stats()\n",
    "        \n",
    "        return self.normalize_state(state), info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "397a9a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom reward shaping\n",
    "class RewardShapingEnv(gym.Wrapper):\n",
    "    \"\"\"Environment wrapper for shaping rewards.\"\"\"\n",
    "    def __init__(self, env):\n",
    "        super(RewardShapingEnv, self).__init__(env)\n",
    "        self.prev_pos = None\n",
    "        self.prev_velocity = None\n",
    "        self.track_progress = 0\n",
    "    \n",
    "    def reset(self, **kwargs):\n",
    "        state, info = self.env.reset(**kwargs)\n",
    "        self.prev_pos = state[:2] \n",
    "        self.prev_velocity = state[2:]\n",
    "        self.track_progress = 0\n",
    "        return state, info\n",
    "    \n",
    "    def step(self, action):\n",
    "        next_state, reward, done, truncated, info = self.env.step(action)\n",
    "        \n",
    "        # Extract current position and velocity\n",
    "        curr_pos = next_state[:2]\n",
    "        curr_velocity = next_state[2:]\n",
    "        \n",
    "        # Basic reward from environment\n",
    "        shaped_reward = reward \n",
    "        \n",
    "        # Reward for making progress on the track (distance traveled in desired direction)\n",
    "        if self.prev_pos is not None:\n",
    "            progress = np.linalg.norm(curr_pos - self.prev_pos)\n",
    "            self.track_progress += progress\n",
    "            shaped_reward += progress * 0.1  # Small reward for progress\n",
    "        \n",
    "        if info[\"rewards\"].get(\"on_road_reward\", True) :\n",
    "            shaped_reward += 0.5\n",
    "        elif info[\"rewards\"].get(\"on_road_reward\", False) :\n",
    "            shaped_reward -= 10\n",
    "\n",
    "        # Reward for maintaining speed\n",
    "        speed = np.linalg.norm(curr_velocity)\n",
    "        shaped_reward += 0.1 * speed \n",
    "        \n",
    "        # Penalty for abrupt changes in velocity (smooth driving)\n",
    "        if self.prev_velocity is not None:\n",
    "            velocity_change = np.linalg.norm(curr_velocity - self.prev_velocity)\n",
    "            shaped_reward -= 0.01 * velocity_change  \n",
    "        \n",
    "        # Additional success reward\n",
    "        if done and reward > 0:\n",
    "            shaped_reward += 20.0  # Bonus for completing the track\n",
    "        \n",
    "        # Update previous values for next step\n",
    "        self.prev_pos = curr_pos\n",
    "        self.prev_velocity = curr_velocity\n",
    "        \n",
    "        return next_state, shaped_reward, done, truncated, info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fd8a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rewards(rewards, window_size=100):\n",
    "    \"\"\"Plot rewards over time with a rolling average.\"\"\"\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(rewards, label='Reward per Episode')\n",
    "    \n",
    "    if len(rewards) >= window_size:\n",
    "        rolling_mean = [np.mean(rewards[max(0, i-window_size):i]) for i in range(1, len(rewards)+1)]\n",
    "        plt.plot(range(len(rolling_mean)), rolling_mean, label=f'{window_size}-Episode Moving Average')\n",
    "    \n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.title('Training Progress')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('training_rewards.png')\n",
    "    plt.close()\n",
    "\n",
    "def evaluate_policy(agent, env, eval_episodes=10):\n",
    "    \"\"\"Evaluate the agent's performance without exploration.\"\"\"\n",
    "    avg_reward = 0.0\n",
    "    successes = 0\n",
    "    \n",
    "    for _ in range(eval_episodes):\n",
    "        state, _ = env.reset()\n",
    "        episode_reward = 0.0\n",
    "        done = False\n",
    "        truncated = False\n",
    "        \n",
    "        while not (done or truncated):\n",
    "            action = agent.select_action(state, evaluate=True)\n",
    "            state, reward, done, truncated, _ = env.step(action)\n",
    "            episode_reward += reward\n",
    "            \n",
    "            if done and reward > 0:\n",
    "                successes += 1\n",
    "        \n",
    "        avg_reward += episode_reward\n",
    "    \n",
    "    avg_reward /= eval_episodes\n",
    "    success_rate = successes / eval_episodes\n",
    "    \n",
    "    print(f\"Evaluation over {eval_episodes} episodes: {avg_reward:.3f} average reward, {success_rate:.2f} success rate\")\n",
    "    return avg_reward, success_rate\n",
    "\n",
    "def train_agent(config):\n",
    "    # Create environment\n",
    "    env = gym.make(\"racetrack-v0\", render_mode=\"rgb_array\")\n",
    "    env.unwrapped.config.update(config)\n",
    "    env.reset()\n",
    "    env = RewardShapingEnv(env)  # Apply reward shaping\n",
    "    env = NormalizedEnv(env)     # Apply observation normalization\n",
    "    \n",
    "    # Get state and action dimensions\n",
    "    print(env.observation_space.shape)\n",
    "    obs_dim = 432\n",
    "    action_dim = env.action_space.shape[0]\n",
    "    action_high = env.action_space.high[0]\n",
    "    \n",
    "    print(f\"State dimension: {obs_dim}\")\n",
    "    print(f\"Action dimension: {action_dim}\")\n",
    "    print(f\"Action high: {action_high}\")\n",
    "\n",
    "    # Hyperparameters\n",
    "    lr = 3e-4\n",
    "    gamma = 0.99\n",
    "    tau = 0.005\n",
    "    alpha = 0.2\n",
    "    max_episodes = 500\n",
    "    max_steps = 100\n",
    "    batch_size = 256\n",
    "    updates_per_step = 1\n",
    "    eval_interval = 20\n",
    "    \n",
    "    # Initialize agent\n",
    "    agent = SAC(\n",
    "        state_dim=obs_dim,\n",
    "        action_dim=action_dim,\n",
    "        action_space=env.action_space,\n",
    "        hidden_dim=256,\n",
    "        lr=lr,\n",
    "        gamma=gamma,\n",
    "        tau=tau,\n",
    "        alpha=alpha,\n",
    "        auto_entropy_tuning=True\n",
    "    )\n",
    "    \n",
    "    # Initialize replay buffer\n",
    "    memory = Memory(capacity=100000)\n",
    "    \n",
    "    # Tracking metrics\n",
    "    rewards = []\n",
    "    eval_rewards = []\n",
    "    eval_success_rates = []\n",
    "    best_eval_reward = -float('inf')\n",
    "    \n",
    "    # Main training loop\n",
    "    total_steps = 0\n",
    "    for episode in range(1, max_episodes + 1):\n",
    "        state, _ = env.reset()\n",
    "        episode_reward = 0\n",
    "        episode_steps = 0\n",
    "        done = False\n",
    "        truncated = False\n",
    "        \n",
    "        while not (done or truncated) and episode_steps < max_steps:\n",
    "            if total_steps < 250:  # Initial exploration\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = agent.select_action(state)\n",
    "            \n",
    "            next_state, reward, done, truncated, _ = env.step(action)\n",
    "            episode_reward += reward\n",
    "            \n",
    "            # Store transition in memory\n",
    "            memory.push(state, action, reward, next_state, float(done))\n",
    "            \n",
    "            # Update agent\n",
    "            if len(memory) > batch_size:\n",
    "                for _ in range(updates_per_step):\n",
    "                    agent.train(memory, batch_size)\n",
    "            \n",
    "            state = next_state\n",
    "            episode_steps += 1\n",
    "            total_steps += 1\n",
    "        \n",
    "        rewards.append(episode_reward)\n",
    "        \n",
    "        # Print episode stats\n",
    "        print(f\"Episode: {episode}, Total Steps: {total_steps}, Reward: {episode_reward:.3f}\")\n",
    "        \n",
    "        # Evaluate and save best model\n",
    "        if episode % eval_interval == 0:\n",
    "            avg_reward, success_rate = evaluate_policy(agent, env)\n",
    "            eval_rewards.append(avg_reward)\n",
    "            eval_success_rates.append(success_rate)\n",
    "            \n",
    "            # Save if it's the best model so far\n",
    "            if avg_reward > best_eval_reward:\n",
    "                best_eval_reward = avg_reward\n",
    "                agent.save('./models/best_model')\n",
    "                print(f\"New best model saved with reward: {best_eval_reward:.3f}\")\n",
    "            \n",
    "            # Save latest model\n",
    "            agent.save('./models/latest_model')\n",
    "            \n",
    "            # Plot training progress\n",
    "            plot_rewards(rewards)\n",
    "    \n",
    "    # Final evaluation\n",
    "    print(\"Training finished. Final evaluation:\")\n",
    "    evaluate_policy(agent, env, eval_episodes=20)\n",
    "    \n",
    "    # Save final model\n",
    "    agent.save('./models/final_model')\n",
    "    \n",
    "    # Close environment\n",
    "    env.close()\n",
    "    \n",
    "    return agent, rewards, eval_rewards, eval_success_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd235289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 12, 12)\n",
      "State dimension: 432\n",
      "Action dimension: 1\n",
      "Action high: 1.0\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 24\u001b[0m\n\u001b[1;32m      1\u001b[0m racing_config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maction\u001b[39m\u001b[38;5;124m'\u001b[39m: {\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlateral\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmanual_control\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     }\n\u001b[0;32m---> 24\u001b[0m agent, rewards, eval_rewards, eval_success_rates \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mracing_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Plot final training results\u001b[39;00m\n\u001b[1;32m     27\u001b[0m plot_rewards(rewards)\n",
      "Cell \u001b[0;32mIn[12], line 110\u001b[0m, in \u001b[0;36mtrain_agent\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    108\u001b[0m     action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mselect_action(state)\n\u001b[0;32m--> 110\u001b[0m next_state, reward, done, truncated, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m episode_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# Store transition in memory\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[14], line 24\u001b[0m, in \u001b[0;36mNormalizedEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     23\u001b[0m     action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(action)\n\u001b[0;32m---> 24\u001b[0m next_state, reward, done, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_buffer\u001b[38;5;241m.\u001b[39mappend(next_state)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize_state(next_state), reward \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreward_scale, done, truncated, info\n",
      "Cell \u001b[0;32mIn[5], line 18\u001b[0m, in \u001b[0;36mRewardShapingEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m---> 18\u001b[0m     next_state, reward, done, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# Extract current position and velocity\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     curr_pos \u001b[38;5;241m=\u001b[39m next_state[:\u001b[38;5;241m2\u001b[39m]\n",
      "File \u001b[0;32m~/RL/venv/lib/python3.10/site-packages/gymnasium/wrappers/common.py:393\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/RL/venv/lib/python3.10/site-packages/gymnasium/core.py:327\u001b[0m, in \u001b[0;36mWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[1;32m    325\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    326\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 327\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/RL/venv/lib/python3.10/site-packages/gymnasium/wrappers/common.py:283\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchecked_step \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchecked_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43menv_step_passive_checker\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[0;32m~/RL/venv/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:207\u001b[0m, in \u001b[0;36menv_step_passive_checker\u001b[0;34m(env, action)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"A passive check for the environment step, investigating the returning data then returning the data unchanged.\"\"\"\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;66;03m# We don't check the action as for some environments then out-of-bounds values can be given\u001b[39;00m\n\u001b[0;32m--> 207\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    209\u001b[0m     result, \u001b[38;5;28mtuple\u001b[39m\n\u001b[1;32m    210\u001b[0m ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpects step result to be a tuple, actual type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(result)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(result) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m:\n",
      "File \u001b[0;32m~/RL/venv/lib/python3.10/site-packages/highway_env/envs/common/abstract.py:240\u001b[0m, in \u001b[0;36mAbstractEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    236\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe road and vehicle must be initialized in the environment implementation\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    237\u001b[0m     )\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolicy_frequency\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 240\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_simulate\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    242\u001b[0m obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_type\u001b[38;5;241m.\u001b[39mobserve()\n\u001b[1;32m    243\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reward(action)\n",
      "File \u001b[0;32m~/RL/venv/lib/python3.10/site-packages/highway_env/envs/common/abstract.py:269\u001b[0m, in \u001b[0;36mAbstractEnv._simulate\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m frame \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(frames):\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;66;03m# Forward action to the vehicle\u001b[39;00m\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    260\u001b[0m         action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    261\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmanual_control\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    268\u001b[0m     ):\n\u001b[0;32m--> 269\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroad\u001b[38;5;241m.\u001b[39mact()\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroad\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimulation_frequency\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/RL/venv/lib/python3.10/site-packages/highway_env/envs/common/action.py:161\u001b[0m, in \u001b[0;36mContinuousAction.act\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mact\u001b[39m(\u001b[38;5;28mself\u001b[39m, action: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 161\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrolled_vehicle\u001b[38;5;241m.\u001b[39mact(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_action \u001b[38;5;241m=\u001b[39m action\n",
      "File \u001b[0;32m~/RL/venv/lib/python3.10/site-packages/highway_env/envs/common/action.py:146\u001b[0m, in \u001b[0;36mContinuousAction.get_action\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    140\u001b[0m     (\n\u001b[1;32m    141\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrolled_vehicle\u001b[38;5;241m.\u001b[39mMIN_SPEED,\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrolled_vehicle\u001b[38;5;241m.\u001b[39mMAX_SPEED,\n\u001b[1;32m    143\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspeed_range\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlongitudinal \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlateral:\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m--> 146\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124macceleration\u001b[39m\u001b[38;5;124m\"\u001b[39m: utils\u001b[38;5;241m.\u001b[39mlmap(\u001b[43maction\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m, [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39macceleration_range),\n\u001b[1;32m    147\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msteering\u001b[39m\u001b[38;5;124m\"\u001b[39m: utils\u001b[38;5;241m.\u001b[39mlmap(action[\u001b[38;5;241m1\u001b[39m], [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteering_range),\n\u001b[1;32m    148\u001b[0m     }\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlongitudinal:\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    151\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124macceleration\u001b[39m\u001b[38;5;124m\"\u001b[39m: utils\u001b[38;5;241m.\u001b[39mlmap(action[\u001b[38;5;241m0\u001b[39m], [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39macceleration_range),\n\u001b[1;32m    152\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msteering\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m    153\u001b[0m     }\n",
      "\u001b[0;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "racing_config = {\n",
    "    'action': {\n",
    "        'lateral': True,\n",
    "        'longitudinal': True,\n",
    "        'target_speeds': [0, 30, 50, 80],\n",
    "        'type': 'ContinuousAction'\n",
    "    },\n",
    "    'duration': 120,\n",
    "    'controlled_vehicles': 1,\n",
    "    'observation': {\n",
    "        'align_to_vehicle_axes': True,\n",
    "        'as_image': False,\n",
    "        'features': ['presence', 'velocity', 'acceleration'],\n",
    "        'grid_size': [[-30, 30], [-30, 30]],\n",
    "        'grid_step': [5, 5],\n",
    "        'type': 'OccupancyGrid'\n",
    "    },\n",
    "    'other_vehicles': 1,\n",
    "    'other_vehicles_type': 'highway_env.vehicle.behavior.AggressiveVehicle',\n",
    "    'render_agent': True,\n",
    "    'manual_control': False\n",
    "    }\n",
    "    \n",
    "agent, rewards, eval_rewards, eval_success_rates = train_agent(config=racing_config)\n",
    "\n",
    "# Plot final training results\n",
    "plot_rewards(rewards)\n",
    "\n",
    "# Plot evaluation metrics\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(list(range(0, len(rewards), 20)), eval_rewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Evaluation Reward')\n",
    "plt.title('Evaluation Rewards')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(list(range(0, len(rewards), 20)), eval_success_rates)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Success Rate')\n",
    "plt.title('Success Rate')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('evaluation_metrics.png')\n",
    "plt.close()\n",
    "\n",
    "# Test the best model\n",
    "print(\"Testing the best model:\")\n",
    "env = gym.make(\"racetrack-v0\", render_mode=\"human\")\n",
    "# env.unwrapped.config.update(racing_config)\n",
    "env = RewardShapingEnv(env)\n",
    "env = NormalizedEnv(env)\n",
    "\n",
    "# Load best model\n",
    "state_dim = 432\n",
    "action_dim = env.action_space.shape[0]\n",
    "best_agent = SAC(state_dim, action_dim, env.action_space)\n",
    "best_agent.load('./models_test/best_model')\n",
    "\n",
    "# Run some test episodes\n",
    "for episode in range(5):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    truncated = False\n",
    "    episode_reward = 0\n",
    "    \n",
    "    while not (done or truncated):\n",
    "        action = best_agent.select_action(state, evaluate=True)\n",
    "        state, reward, done, truncated, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "    \n",
    "    print(f\"Test Episode {episode+1}: Reward = {episode_reward:.3f}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9330ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "\n",
    "import highway_env  # noqa: F401\n",
    "\n",
    "\n",
    "TRAIN = True\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_cpu = 6\n",
    "    batch_size = 64\n",
    "    env = make_vec_env(\"racetrack-v0\", n_envs=n_cpu, vec_env_cls=SubprocVecEnv)\n",
    "    model = PPO(\n",
    "        \"MlpPolicy\",\n",
    "        env,\n",
    "        policy_kwargs=dict(net_arch=[dict(pi=[256, 256], vf=[256, 256])]),\n",
    "        n_steps=batch_size * 12 // n_cpu,\n",
    "        batch_size=batch_size,\n",
    "        n_epochs=10,\n",
    "        learning_rate=5e-4,\n",
    "        gamma=0.9,\n",
    "        verbose=2,\n",
    "    )\n",
    "    # Train the model\n",
    "    if TRAIN:\n",
    "        model.learn(total_timesteps=int(1e5))\n",
    "        model.save(\"racetrack_ppo/model\")\n",
    "        del model\n",
    "\n",
    "    # Run the algorithm\n",
    "    model = PPO.load(\"racetrack_ppo/model\", env=env)\n",
    "\n",
    "    env = gym.make(\"racetrack-v0\")\n",
    "\n",
    "    for episode in range(10):\n",
    "        done = truncated = False\n",
    "        obs, info = env.reset()\n",
    "        while not (done or truncated):\n",
    "            # Predict\n",
    "            action, _states = model.predict(obs, deterministic=True)\n",
    "            # Get reward\n",
    "            obs, reward, done, truncated, info = env.step(action)\n",
    "            # Render\n",
    "            env.render()\n",
    "    env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
