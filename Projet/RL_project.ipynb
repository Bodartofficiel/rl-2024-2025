{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'équipe pour réaliser ce projet est constituée de : Thomas Bodart, Maceo Duriez et Marc-César Garcia-Grenet\n",
    "\n",
    "Vous trouverez ci-dessous les deux configurations (continuous_actions et stable_baselines) que nous avons retenues pour le projet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import highway_env\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from copy import deepcopy\n",
    "from collections import deque\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "def run_one_episode(env, agent, display=True):\n",
    "    display_env = deepcopy(env)\n",
    "    done = False\n",
    "    state, _ = display_env.reset()\n",
    "\n",
    "    rewards = 0\n",
    "\n",
    "    while not done:\n",
    "        action = agent.get_action(state, epsilon=0)\n",
    "        state, reward, terminated, truncated, _ = display_env.step(action)\n",
    "        done = terminated or truncated\n",
    "        rewards += reward\n",
    "        if display: \n",
    "            clear_output(wait=True)\n",
    "            plt.imshow(display_env.render())\n",
    "            plt.show()\n",
    "    if display:\n",
    "        display_env.close()\n",
    "    print(f'total episode reward {rewards}')\n",
    "\n",
    "def eval_agent(agent, env, n_sim=10):\n",
    "    \"\"\"    \n",
    "    Monte Carlo evaluation of the agent.\n",
    "\n",
    "    Repeat n_sim times:\n",
    "        * Run the agent policy until the environment reaches a terminal state (= one episode)\n",
    "        * Compute the sum of rewards in this episode\n",
    "        * Store the sum of rewards in the episode_rewards array.\n",
    "    \"\"\"\n",
    "    env_copy = deepcopy(env)\n",
    "    episode_rewards = np.zeros(n_sim)\n",
    "    for i in range(n_sim):\n",
    "        state, _ = env_copy.reset()\n",
    "        reward_sum = 0\n",
    "        done = False\n",
    "        while not done: \n",
    "            action = agent.get_action(state, epsilon=0)\n",
    "            state, reward, terminated, truncated, _ = env_copy.step(action)\n",
    "            reward_sum += reward\n",
    "            done = terminated or truncated\n",
    "        episode_rewards[i] = reward_sum\n",
    "    return episode_rewards\n",
    "\n",
    "class RandomAgent: \n",
    "    def __init__(self, observation_space, action_space):\n",
    "        self.action_space = action_space\n",
    "        return\n",
    "    \n",
    "    def get_action(self, state, **kwargs):\n",
    "        return self.action_space.sample()\n",
    "    \n",
    "    def update(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some utilities functions: train, curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, agent, N_episodes, eval_every=100, reward_threshold=300, n_eval=10, save_model=False):\n",
    "    total_time = 0\n",
    "    all_rewards = []\n",
    "    all_lengths = []\n",
    "    eval_rewards = []\n",
    "    \n",
    "    for ep in tqdm(range(N_episodes), desc=\"Training...\"):\n",
    "        done = False\n",
    "        state, _ = env.reset()\n",
    "        ep_reward = 0\n",
    "        steps = 0\n",
    "\n",
    "        while not done: \n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            \n",
    "            agent.store_transition(state, action, reward, next_state, terminated)\n",
    "            agent.update()\n",
    "\n",
    "            state = next_state\n",
    "            done = terminated or truncated\n",
    "\n",
    "            ep_reward += reward\n",
    "            steps += 1\n",
    "            total_time += 1\n",
    "\n",
    "        all_rewards.append(ep_reward)\n",
    "        all_lengths.append(steps)\n",
    "\n",
    "        if (ep + 1) % eval_every == 0:\n",
    "            mean_eval_reward = np.mean(eval_agent(agent, env, n_sim=n_eval))\n",
    "            eval_rewards.append((ep + 1, mean_eval_reward))\n",
    "            print(f\"[Episode {ep+1}] Eval reward: {mean_eval_reward:.2f} | Train reward: {ep_reward:.2f} | Steps: {steps}\")\n",
    "            \n",
    "            if save_model:\n",
    "                torch.save(agent.policy_net.state_dict(), f\"model_ep{ep+1}.pt\")\n",
    "\n",
    "            if mean_eval_reward >= reward_threshold:\n",
    "                print(\"Reward threshold reached! Stopping early.\")\n",
    "                break\n",
    "\n",
    "    print(\"\\n Training complete.\")\n",
    "    _plot_training_curves(all_rewards, eval_rewards)\n",
    "    return all_rewards, eval_rewards, all_lengths\n",
    "\n",
    "\n",
    "def _plot_training_curves(all_rewards, eval_rewards):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Smoothed training reward\n",
    "    smoothed_rewards = np.convolve(all_rewards, np.ones(10) / 10, mode='valid')\n",
    "    plt.plot(smoothed_rewards, label='Train Reward (smoothed)', alpha=0.7)\n",
    "\n",
    "    if eval_rewards:\n",
    "        eval_eps, eval_vals = zip(*eval_rewards)\n",
    "        plt.plot(eval_eps, eval_vals, 'o-', label='Eval Reward', color='red')\n",
    "\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.title('Training and Evaluation Reward Over Time')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Pre-specified environment\n",
    "Let us first define the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total episode reward 2.3078428508526816\n"
     ]
    }
   ],
   "source": [
    "\n",
    "config_dict = {\n",
    "    \"observation\": {\n",
    "        \"type\": \"OccupancyGrid\",\n",
    "        \"vehicles_count\": 10,\n",
    "        \"features\": [\"presence\", \"x\", \"y\", \"vx\", \"vy\", \"cos_h\", \"sin_h\"],\n",
    "        \"features_range\": {\n",
    "            \"x\": [-100, 100],\n",
    "            \"y\": [-100, 100],\n",
    "            \"vx\": [-20, 20],\n",
    "            \"vy\": [-20, 20],\n",
    "        },\n",
    "        \"grid_size\": [[-20, 20], [-20, 20]],\n",
    "        \"grid_step\": [5, 5],\n",
    "        \"absolute\": False,\n",
    "    },\n",
    "    \"action\": {\n",
    "        \"type\": \"DiscreteMetaAction\",\n",
    "    },\n",
    "    \"lanes_count\": 4,\n",
    "    \"vehicles_count\": 15,\n",
    "    \"duration\": 60,  # [s]\n",
    "    \"initial_spacing\": 0,\n",
    "    \"collision_reward\": -1,  # The reward received when colliding with a vehicle.\n",
    "    \"right_lane_reward\": 0.5,  # The reward received when driving on the right-most lanes, linearly mapped to\n",
    "    # zero for other lanes.\n",
    "    \"high_speed_reward\": 0.1,  # The reward received when driving at full speed, linearly mapped to zero for\n",
    "    # lower speeds according to config[\"reward_speed_range\"].\n",
    "    \"lane_change_reward\": 0,\n",
    "    \"reward_speed_range\": [\n",
    "        20,\n",
    "        30,\n",
    "    ],  # [m/s] The reward for high speed is mapped linearly from this range to [0, HighwayEnv.HIGH_SPEED_REWARD].\n",
    "    \"simulation_frequency\": 5,  # [Hz]\n",
    "    \"policy_frequency\": 1,  # [Hz]\n",
    "    \"other_vehicles_type\": \"highway_env.vehicle.behavior.IDMVehicle\",\n",
    "    \"screen_width\": 600,  # [px]\n",
    "    \"screen_height\": 150,  # [px]\n",
    "    \"centering_position\": [0.3, 0.5],\n",
    "    \"scaling\": 5.5,\n",
    "    \"show_trajectories\": True,\n",
    "    \"render_agent\": True,\n",
    "    \"offscreen_rendering\": False,\n",
    "    \"disable_collision_checks\": True,\n",
    "}\n",
    "\n",
    "env = gym.make(\"highway-fast-v0\", render_mode=\"rgb_array\", config = config_dict)\n",
    "env.reset()\n",
    "\n",
    "\n",
    "run_one_episode(env, RandomAgent(env.observation_space, env.action_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General comments on the environment\n",
    "### Observation space:\n",
    "The observation space is a 8x8 grid, with the following channels (as in image):\n",
    "- Presence: 1 if a car is here, 0 otherwise\n",
    "- x and y: relative position\n",
    "- vx, vy: relative velocities\n",
    "- cos_h, sin_h: angle\n",
    "\n",
    "### Action space:\n",
    "There is 5 possible actions:\n",
    "- IDLE: do nothing\n",
    "- LEFT: go left\n",
    "- RIGHT: go right\n",
    "- FASTER: go faster\n",
    "- SLOWER: go slower\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space shape:  (7, 8, 8)\n",
      "Action space shape Discrete(5)\n"
     ]
    }
   ],
   "source": [
    "print(\"Observation space shape: \", env.observation_space.shape)\n",
    "print(\"Action space shape\", env.action_space)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN Implementation\n",
    "### Neural Network\n",
    "Let us define the neural network for DQN. Since the observation space looks like an 8x8 image (with 7 channels), we will use CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0074, -0.0650,  0.0230,  0.2056,  0.1279]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.conv1 = nn.Conv2d(in_channels=7, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.ln1 = nn.LayerNorm([16, 8, 8])\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.ln2 = nn.LayerNorm([32, 8, 8])\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 128)\n",
    "        self.fc2 = nn.Linear(128, 5)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.flatten = nn.Flatten()\n",
    "    \n",
    "    def forward(self, observation_space: torch.FloatTensor):\n",
    "        x = self.relu(self.ln1(self.conv1(observation_space)))\n",
    "        x = self.relu(self.ln2(self.conv2(x)))\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "ex = torch.tensor([env.observation_space.sample()])\n",
    "model = CNN()\n",
    "model(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent\n",
    "Let us now define the agent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=100_000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, *transition):\n",
    "        self.buffer.append(transition)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        action_space,\n",
    "        observation_space,\n",
    "        gamma,\n",
    "        batch_size,\n",
    "        learning_rate,\n",
    "        weight_decay,\n",
    "        model,\n",
    "        optimizer,\n",
    "        buffer_capacity\n",
    "    ):\n",
    "        self.action_space = action_space\n",
    "        self.observation_space = observation_space\n",
    "        self.n_actions = action_space.n\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "        self.policy_net = model.to(self.device)\n",
    "        self.target_net = deepcopy(model).to(self.device)\n",
    "        self.target_net.eval()\n",
    "\n",
    "        self.optimizer = optimizer(self.policy_net.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        self.replay_buffer = ReplayBuffer(capacity=buffer_capacity)\n",
    "\n",
    "        self.steps = 0\n",
    "        self.update_target_every = 100\n",
    "\n",
    "        self.action_counter = Counter()\n",
    "\n",
    "    def update(self):\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        transitions = self.replay_buffer.sample(self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*transitions)\n",
    "\n",
    "        states = torch.tensor(states, device=self.device, dtype=torch.float32)\n",
    "        actions = torch.tensor(actions, device=self.device).unsqueeze(1)\n",
    "        rewards = torch.tensor(rewards, device=self.device, dtype=torch.float32).unsqueeze(1)\n",
    "        next_states = torch.tensor(next_states, device=self.device, dtype=torch.float32)\n",
    "        dones = torch.tensor(dones, device=self.device, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "        q_values = self.policy_net(states).gather(1, actions)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.target_net(next_states).max(1, keepdim=True)[0]\n",
    "            target_q_values = rewards + self.gamma * next_q_values * (1 - dones)\n",
    "\n",
    "        loss = torch.nn.functional.l1_loss(q_values, target_q_values)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.steps += 1\n",
    "        if self.steps % self.update_target_every == 0:\n",
    "            self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        self.replay_buffer.push(state, action, reward, next_state, done)\n",
    "\n",
    "    def get_action(self, state, epsilon=0.1):\n",
    "        if random.random() < epsilon:\n",
    "            action = self.action_space.sample()\n",
    "            self.action_counter[action] += 1\n",
    "            return action \n",
    "\n",
    "        state_tensor = torch.tensor(state, device=self.device, dtype=torch.float32).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.policy_net(state_tensor)\n",
    "        \n",
    "        action = q_values.argmax().item()\n",
    "        self.action_counter[action] += 1\n",
    "        return action\n",
    "        \n",
    "    def reset_action_counter(self):\n",
    "        self.action_counter = Counter()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward before training =  37.7708726960574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:   7%|▋         | 37/500 [00:15<03:07,  2.46it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[173]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mmean reward before training = \u001b[39m\u001b[33m\"\u001b[39m, np.mean(eval_agent(agent, env, \u001b[32m10\u001b[39m)))\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Run the training loop\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_episodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_every\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# Evaluate the final policy\u001b[39;00m\n\u001b[32m     32\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mmean reward after training = \u001b[39m\u001b[33m\"\u001b[39m, np.mean(eval_agent(agent, env, \u001b[32m10\u001b[39m)))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[166]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(env, agent, N_episodes, eval_every, reward_threshold, n_eval, save_model)\u001b[39m\n\u001b[32m     15\u001b[39m next_state, reward, terminated, truncated, _ = env.step(action)\n\u001b[32m     17\u001b[39m agent.store_transition(state, action, reward, next_state, terminated)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m state = next_state\n\u001b[32m     21\u001b[39m done = terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[172]\u001b[39m\u001b[32m, line 54\u001b[39m, in \u001b[36mDQNAgent.update\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     51\u001b[39m transitions = \u001b[38;5;28mself\u001b[39m.replay_buffer.sample(\u001b[38;5;28mself\u001b[39m.batch_size)\n\u001b[32m     52\u001b[39m states, actions, rewards, next_states, dones = \u001b[38;5;28mzip\u001b[39m(*transitions)\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m states = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m actions = torch.tensor(actions, device=\u001b[38;5;28mself\u001b[39m.device).unsqueeze(\u001b[32m1\u001b[39m)\n\u001b[32m     56\u001b[39m rewards = torch.tensor(rewards, device=\u001b[38;5;28mself\u001b[39m.device, dtype=torch.float32).unsqueeze(\u001b[32m1\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "env = gym.make(\"highway-fast-v0\", render_mode=\"rgb_array\", config = config_dict)\n",
    "env.reset()\n",
    "\n",
    "action_space = env.action_space\n",
    "observation_space = env.observation_space\n",
    "\n",
    "gamma = .99\n",
    "episode_batch_size = 64\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 0\n",
    "model=CNN()\n",
    "optimizer = torch.optim.Adam\n",
    "N_episodes = 500\n",
    "\n",
    "agent = DQNAgent(\n",
    "    action_space=action_space,\n",
    "    observation_space=observation_space,\n",
    "    gamma=gamma,\n",
    "    batch_size=episode_batch_size,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    buffer_capacity=1000,\n",
    ")\n",
    "\n",
    "print(\"mean reward before training = \", np.mean(eval_agent(agent, env, 10)))\n",
    "# Run the training loop\n",
    "train(env, agent, N_episodes, eval_every=50,)\n",
    "\n",
    "# Evaluate the final policy\n",
    "print(\"mean reward after training = \", np.mean(eval_agent(agent, env, 10)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total episode reward 55.764875091613675\n",
      "Counter()\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "\n",
    "agent.reset_action_counter()\n",
    "\n",
    "run_one_episode(env, agent)\n",
    "\n",
    "print(agent.action_counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m100\u001b[39m):\n\u001b[32m     40\u001b[39m     action = env.action_space.sample()\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m     obs, reward, done, truncated, info = \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m     env.render()\n\u001b[32m     44\u001b[39m plt.imshow(env.render())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CS_cours/rl-2024--2025/.venv/lib/python3.12/site-packages/gymnasium/wrappers/common.py:393\u001b[39m, in \u001b[36mOrderEnforcing.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    391\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._has_reset:\n\u001b[32m    392\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[33m\"\u001b[39m\u001b[33mCannot call env.step() before calling env.reset()\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m393\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CS_cours/rl-2024--2025/.venv/lib/python3.12/site-packages/gymnasium/core.py:327\u001b[39m, in \u001b[36mWrapper.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    323\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\n\u001b[32m    324\u001b[39m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[32m    325\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[32m    326\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CS_cours/rl-2024--2025/.venv/lib/python3.12/site-packages/gymnasium/wrappers/common.py:285\u001b[39m, in \u001b[36mPassiveEnvChecker.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    283\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m.env, action)\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CS_cours/rl-2024--2025/.venv/lib/python3.12/site-packages/highway_env/envs/common/abstract.py:240\u001b[39m, in \u001b[36mAbstractEnv.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    235\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[32m    236\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe road and vehicle must be initialized in the environment implementation\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    237\u001b[39m     )\n\u001b[32m    239\u001b[39m \u001b[38;5;28mself\u001b[39m.time += \u001b[32m1\u001b[39m / \u001b[38;5;28mself\u001b[39m.config[\u001b[33m\"\u001b[39m\u001b[33mpolicy_frequency\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_simulate\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    242\u001b[39m obs = \u001b[38;5;28mself\u001b[39m.observation_type.observe()\n\u001b[32m    243\u001b[39m reward = \u001b[38;5;28mself\u001b[39m._reward(action)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CS_cours/rl-2024--2025/.venv/lib/python3.12/site-packages/highway_env/envs/common/abstract.py:280\u001b[39m, in \u001b[36mAbstractEnv._simulate\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    275\u001b[39m     \u001b[38;5;66;03m# Automatically render intermediate simulation steps if a viewer has been launched\u001b[39;00m\n\u001b[32m    276\u001b[39m     \u001b[38;5;66;03m# Ignored if the rendering is done offscreen\u001b[39;00m\n\u001b[32m    277\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    278\u001b[39m         frame < frames - \u001b[32m1\u001b[39m\n\u001b[32m    279\u001b[39m     ):  \u001b[38;5;66;03m# Last frame will be rendered through env.render() as usual\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m280\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_automatic_rendering\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    282\u001b[39m \u001b[38;5;28mself\u001b[39m.enable_auto_render = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CS_cours/rl-2024--2025/.venv/lib/python3.12/site-packages/highway_env/envs/common/abstract.py:340\u001b[39m, in \u001b[36mAbstractEnv._automatic_rendering\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    338\u001b[39m     \u001b[38;5;28mself\u001b[39m._record_video_wrapper.video_recorder.capture_frame()\n\u001b[32m    339\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m340\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CS_cours/rl-2024--2025/.venv/lib/python3.12/site-packages/highway_env/envs/common/abstract.py:303\u001b[39m, in \u001b[36mAbstractEnv.render\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    299\u001b[39m     \u001b[38;5;28mself\u001b[39m.viewer = EnvViewer(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m    301\u001b[39m \u001b[38;5;28mself\u001b[39m.enable_auto_render = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m303\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mviewer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdisplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    305\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.viewer.offscreen:\n\u001b[32m    306\u001b[39m     \u001b[38;5;28mself\u001b[39m.viewer.handle_events()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CS_cours/rl-2024--2025/.venv/lib/python3.12/site-packages/highway_env/envs/common/graphics.py:159\u001b[39m, in \u001b[36mEnvViewer.display\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    157\u001b[39m     \u001b[38;5;28mself\u001b[39m.screen.blit(\u001b[38;5;28mself\u001b[39m.sim_surface, (\u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m))\n\u001b[32m    158\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.env.config[\u001b[33m\"\u001b[39m\u001b[33mreal_time_rendering\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m159\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msimulation_frequency\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    160\u001b[39m     pygame.display.flip()\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.SAVE_IMAGES \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.directory:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "racing_config = {\n",
    "    'action': {\n",
    "        'lateral': True,\n",
    "        'longitudinal': True,\n",
    "        'target_speeds': [0, 30, 50, 80],\n",
    "        'type': 'ContinuousAction'\n",
    "    },\n",
    "    'action_reward': -0.1,\n",
    "    'collision_reward': -5,\n",
    "    'controlled_vehicles': 1,\n",
    "    'duration': 1200,\n",
    "    'lane_centering_cost': 1,\n",
    "    'lane_centering_reward': 0.1,\n",
    "    'manual_control': False,\n",
    "    'observation': {\n",
    "        'align_to_vehicle_axes': True,\n",
    "        'as_image': False,\n",
    "        'features': ['presence', 'velocity', 'acceleration'],\n",
    "        'grid_size': [[-30, 30], [-30, 30]],\n",
    "        'grid_step': [5, 5],\n",
    "        'type': 'OccupancyGrid'\n",
    "    },\n",
    "    'offscreen_rendering': False,\n",
    "    'other_vehicles': 3, \n",
    "    'other_vehicles_type': 'highway_env.vehicle.behavior.AggressiveVehicle',\n",
    "    'policy_frequency': 5,\n",
    "    'real_time_rendering': True,\n",
    "    'render_agent': True,\n",
    "    'scaling': 6,\n",
    "    'screen_height': 800,\n",
    "    'screen_width': 1200,\n",
    "    'show_trajectories': False,\n",
    "    'simulation_frequency': 15,\n",
    "}\n",
    "\n",
    "env = gym.make(\"racetrack-v0\", render_mode=\"rgb_array\", config=racing_config)\n",
    "env.reset()\n",
    "\n",
    "for _ in range(100):\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "    env.render()\n",
    "\n",
    "plt.imshow(env.render())\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'get_image'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 40\u001b[39m\n\u001b[32m     38\u001b[39m     action = env.action_space.sample()\n\u001b[32m     39\u001b[39m     obs, reward, done, truncated, info = env.step(action)\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m plt.imshow(env.render())\n\u001b[32m     43\u001b[39m env.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CS_cours/rl-2024--2025/.venv/lib/python3.12/site-packages/gymnasium/wrappers/common.py:409\u001b[39m, in \u001b[36mOrderEnforcing.render\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    404\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._disable_render_order_enforcing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._has_reset:\n\u001b[32m    405\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\n\u001b[32m    406\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCannot call `env.render()` before calling `env.reset()`, if this is an intended action, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    407\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mset `disable_render_order_enforcing=True` on the OrderEnforcer wrapper.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    408\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m409\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CS_cours/rl-2024--2025/.venv/lib/python3.12/site-packages/gymnasium/core.py:337\u001b[39m, in \u001b[36mWrapper.render\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    335\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> RenderFrame | \u001b[38;5;28mlist\u001b[39m[RenderFrame] | \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    336\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Uses the :meth:`render` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m337\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CS_cours/rl-2024--2025/.venv/lib/python3.12/site-packages/gymnasium/wrappers/common.py:303\u001b[39m, in \u001b[36mPassiveEnvChecker.render\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    301\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m env_render_passive_checker(\u001b[38;5;28mself\u001b[39m.env)\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m303\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CS_cours/rl-2024--2025/.venv/lib/python3.12/site-packages/highway_env/envs/common/abstract.py:308\u001b[39m, in \u001b[36mAbstractEnv.render\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    306\u001b[39m     \u001b[38;5;28mself\u001b[39m.viewer.handle_events()\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.render_mode == \u001b[33m\"\u001b[39m\u001b[33mrgb_array\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m308\u001b[39m     image = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mviewer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_image\u001b[49m()\n\u001b[32m    309\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m image\n",
      "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'get_image'"
     ]
    }
   ],
   "source": [
    "stablebaselines_config = {'action': {'type': 'DiscreteMetaAction'},\n",
    "'centering_position': [0.3, 0.5],\n",
    "'collision_reward': -1.0,\n",
    "'controlled_vehicles': 1,\n",
    "'duration': 45,\n",
    "'ego_spacing': 2,\n",
    "'high_speed_reward': 1.0,\n",
    "'initial_lane_id': None,\n",
    "'lane_change_reward': 0.2,\n",
    "'lanes_count': 4,\n",
    "'manual_control': False,\n",
    "'normalize_reward': True,\n",
    "'observation': {'type': 'Kinematics'},\n",
    "'offroad_terminal': False,\n",
    "'offscreen_rendering': False,\n",
    "'other_vehicles_type': 'highway_env.vehicle.behavior.AggressiveVehicle',\n",
    "'policy_frequency': 1,\n",
    "'real_time_rendering': False,\n",
    "'render_agent': True,\n",
    "'reward_speed_range': [30, 80],\n",
    "'right_lane_reward': -0.1,\n",
    "'scaling': 6.0,\n",
    "'screen_height': 300,\n",
    "'screen_width': 1200,\n",
    "'show_trajectories': False,\n",
    "'simulation_frequency': 15,\n",
    "'vehicles_count': 100,\n",
    "'vehicles_density': 2}\n",
    "\n",
    "env = gym.make(\"highway-fast-v0\", render_mode=\"rgb_array\", config=stablebaselines_config)\n",
    "env.reset()\n",
    "\n",
    "for _ in range(100):\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "    env.render()\n",
    "\n",
    "plt.imshow(env.render())\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
