{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'équipe pour réaliser ce projet est constituée de : Thomas Bodart, Maceo Duriez et Marc-César Garcia-Grenet\n",
    "\n",
    "Vous trouverez ci-dessous les deux configurations (continuous_actions et stable_baselines) que nous avons retenues pour le projet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import highway_env\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "def run_one_episode(env, agent, display=True):\n",
    "    display_env = deepcopy(env)\n",
    "    done = False\n",
    "    state, _ = display_env.reset()\n",
    "\n",
    "    rewards = 0\n",
    "\n",
    "    while not done:\n",
    "        action = agent.get_action(state, epsilon=0)\n",
    "        state, reward, done, _, _ = display_env.step(action)\n",
    "        rewards += reward\n",
    "        if display: \n",
    "            clear_output(wait=True)\n",
    "            plt.imshow(display_env.render())\n",
    "            plt.show()\n",
    "    if display:\n",
    "        display_env.close()\n",
    "    print(f'Episode length {rewards}')\n",
    "\n",
    "def eval_agent(agent, env, n_sim=10):\n",
    "    \"\"\"    \n",
    "    Monte Carlo evaluation of the agent.\n",
    "\n",
    "    Repeat n_sim times:\n",
    "        * Run the agent policy until the environment reaches a terminal state (= one episode)\n",
    "        * Compute the sum of rewards in this episode\n",
    "        * Store the sum of rewards in the episode_rewards array.\n",
    "    \"\"\"\n",
    "    env_copy = deepcopy(env)\n",
    "    episode_rewards = np.zeros(n_sim)\n",
    "    for i in range(n_sim):\n",
    "        state, _ = env_copy.reset()\n",
    "        reward_sum = 0\n",
    "        done = False\n",
    "        while not done: \n",
    "            action = agent.get_action(state, epsilon=0)\n",
    "            state, reward, terminated, truncated, _ = env_copy.step(action)\n",
    "            reward_sum += reward\n",
    "            done = terminated or truncated\n",
    "        episode_rewards[i] = reward_sum\n",
    "    return episode_rewards\n",
    "\n",
    "def train(env, agent, N_episodes, eval_every=100, reward_threshold=300, n_eval=10):\n",
    "    total_time = 0\n",
    "    for ep in tqdm(range(N_episodes), desc=\"Training...\"):\n",
    "        done = False\n",
    "        state, _ = env.reset()\n",
    "        while not done: \n",
    "            action = agent.get_action(state)\n",
    "\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            agent.update(state, action, reward, terminated, next_state)\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            done = terminated or truncated\n",
    "            total_time += 1\n",
    "\n",
    "        if ((ep+1)% eval_every == 0):\n",
    "            mean_reward = np.mean(eval_agent(agent, env, n_sim=n_eval))\n",
    "            print(\"episode =\", ep+1, \", reward = \", mean_reward)\n",
    "            if mean_reward >= reward_threshold:\n",
    "                break\n",
    "                \n",
    "    return \n",
    "\n",
    "class RandomAgent: \n",
    "    def __init__(self, observation_space, action_space):\n",
    "        self.action_space = action_space\n",
    "        return\n",
    "    \n",
    "    def get_action(self, state, **kwargs):\n",
    "        return self.action_space.sample()\n",
    "    \n",
    "    def update(self, *data):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Pre-specified environment\n",
    "Let us first define the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode length 13.526325725238394\n"
     ]
    }
   ],
   "source": [
    "\n",
    "config_dict = {\n",
    "    \"observation\": {\n",
    "        \"type\": \"OccupancyGrid\",\n",
    "        \"vehicles_count\": 10,\n",
    "        \"features\": [\"presence\", \"x\", \"y\", \"vx\", \"vy\", \"cos_h\", \"sin_h\"],\n",
    "        \"features_range\": {\n",
    "            \"x\": [-100, 100],\n",
    "            \"y\": [-100, 100],\n",
    "            \"vx\": [-20, 20],\n",
    "            \"vy\": [-20, 20],\n",
    "        },\n",
    "        \"grid_size\": [[-20, 20], [-20, 20]],\n",
    "        \"grid_step\": [5, 5],\n",
    "        \"absolute\": False,\n",
    "    },\n",
    "    \"action\": {\n",
    "        \"type\": \"DiscreteMetaAction\",\n",
    "    },\n",
    "    \"lanes_count\": 4,\n",
    "    \"vehicles_count\": 15,\n",
    "    \"duration\": 60,  # [s]\n",
    "    \"initial_spacing\": 0,\n",
    "    \"collision_reward\": -1,  # The reward received when colliding with a vehicle.\n",
    "    \"right_lane_reward\": 0.5,  # The reward received when driving on the right-most lanes, linearly mapped to\n",
    "    # zero for other lanes.\n",
    "    \"high_speed_reward\": 0.1,  # The reward received when driving at full speed, linearly mapped to zero for\n",
    "    # lower speeds according to config[\"reward_speed_range\"].\n",
    "    \"lane_change_reward\": 0,\n",
    "    \"reward_speed_range\": [\n",
    "        20,\n",
    "        30,\n",
    "    ],  # [m/s] The reward for high speed is mapped linearly from this range to [0, HighwayEnv.HIGH_SPEED_REWARD].\n",
    "    \"simulation_frequency\": 5,  # [Hz]\n",
    "    \"policy_frequency\": 1,  # [Hz]\n",
    "    \"other_vehicles_type\": \"highway_env.vehicle.behavior.IDMVehicle\",\n",
    "    \"screen_width\": 600,  # [px]\n",
    "    \"screen_height\": 150,  # [px]\n",
    "    \"centering_position\": [0.3, 0.5],\n",
    "    \"scaling\": 5.5,\n",
    "    \"show_trajectories\": True,\n",
    "    \"render_agent\": True,\n",
    "    \"offscreen_rendering\": False,\n",
    "    \"disable_collision_checks\": True,\n",
    "}\n",
    "\n",
    "env = gym.make(\"highway-fast-v0\", render_mode=\"rgb_array\", config = config_dict)\n",
    "env.reset()\n",
    "\n",
    "\n",
    "run_one_episode(env, RandomAgent(env.observation_space, env.action_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General comments on the environment\n",
    "### Observation space:\n",
    "The observation space is a 8x8 grid, with the following channels (as in image):\n",
    "- Presence: 1 if a car is here, 0 otherwise\n",
    "- x and y: relative position\n",
    "- vx, vy: relative velocities\n",
    "- cos_h, sin_h: angle\n",
    "\n",
    "### Action space:\n",
    "There is 5 possible actions:\n",
    "- IDLE: do nothing\n",
    "- LEFT: go left\n",
    "- RIGHT: go right\n",
    "- FASTER: go faster\n",
    "- SLOWER: go slower\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space shape:  (7, 8, 8)\n",
      "Action space shape Discrete(5)\n"
     ]
    }
   ],
   "source": [
    "print(\"Observation space shape: \", env.observation_space.shape)\n",
    "print(\"Action space shape\", env.action_space)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN Implementation\n",
    "### Neural Network\n",
    "Let us define the neural network for DQN. Since the observation space looks like an 8x8 image (with 7 channels), we will use CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c1/j3mjhbjn00x3fcz1qgkn682h0000gn/T/ipykernel_36178/1167295926.py:22: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:257.)\n",
      "  ex = torch.tensor([env.observation_space.sample()])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0053, -0.0535, -0.0043,  0.0244, -0.0535]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.conv1 = nn.Conv2d(in_channels=7, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 128)\n",
    "        self.fc2 = nn.Linear(128, 5)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.flatten = nn.Flatten()\n",
    "    \n",
    "    def forward(self, observation_space:torch.FloatTensor):\n",
    "        x = self.relu(self.conv1(observation_space))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.flatten(self.flatten(x))\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "ex = torch.tensor([env.observation_space.sample()])\n",
    "model = CNN()\n",
    "model(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent\n",
    "Let us now define the agent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        action_space,\n",
    "        observation_space,\n",
    "        gamma,\n",
    "        episode_batch_size,\n",
    "        learning_rate,\n",
    "        weight_decay,\n",
    "        model,\n",
    "        optimizer,\n",
    "    ):\n",
    "        self.action_space = action_space\n",
    "        self.observation_space = observation_space\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        self.n_eps = 0\n",
    "        self.scores = []\n",
    "        self.current_episode = []\n",
    "\n",
    "\n",
    "        self.episode_batch_size = episode_batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.device = torch.device(\"cuda\") if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "        self.policy_net = model.to(self.device)\n",
    "        \n",
    "\n",
    "        self.optimizer = optimizer(self.policy_net.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    \n",
    "    def _gradient_returns_with_mean_goal(self, rewards, gamma):\n",
    "        \"\"\"\n",
    "        Turns a list of rewards into the list of returns * gamma**t\n",
    "        \"\"\"\n",
    "        mean_goal = self._mean_goal(rewards, gamma)\n",
    "        G = 0\n",
    "        returns_list = []\n",
    "        T = len(rewards)\n",
    "        full_gamma = np.power(gamma, T)\n",
    "        for t in range(T):\n",
    "            G = rewards[T-t-1] + gamma * G\n",
    "            full_gamma /= gamma\n",
    "            returns_list.append(full_gamma * (G - mean_goal))\n",
    "        return torch.tensor(returns_list[::-1]).to(self.device)\n",
    "    \n",
    "    def _mean_goal(self, rewards, gamma):\n",
    "        T = len(rewards)\n",
    "        G = torch.zeros(T)\n",
    "        for t in range(T-2, -1, -1):\n",
    "            G[t] = rewards[t + 1] + gamma * G[t + 1]\n",
    "        return G.mean()\n",
    "\n",
    "    def update(self, state, action, reward, terminated, next_state):\n",
    "        self.current_episode.append((\n",
    "            torch.tensor(state, device=self.device).unsqueeze(0),\n",
    "            torch.tensor([[action]], dtype=torch.int64, device=self.device),\n",
    "            torch.tensor([reward], dtype=torch.float32, device=self.device),\n",
    "        )\n",
    "        )\n",
    "\n",
    "        if terminated:\n",
    "            states, actions, rewards = [torch.cat(a) for a in zip(*self.current_episode)]\n",
    "            returns = self._gradient_returns_with_mean_goal(rewards, self.gamma)\n",
    "\n",
    "            logits = self.policy_net(states)\n",
    "            log_probs = logits.log_softmax(dim=-1)\n",
    "            selected_log_probs = torch.gather(log_probs, 1, actions).squeeze()\n",
    "\n",
    "            if not hasattr(self, 'batch_returns'):\n",
    "                self.batch_returns = []\n",
    "                self.batch_log_probs = []\n",
    "\n",
    "            self.batch_returns.extend(returns)\n",
    "            self.batch_log_probs.extend(selected_log_probs)\n",
    "\n",
    "\n",
    "            if (self.n_eps % self.episode_batch_size) == 0:\n",
    "                batch_returns = torch.stack(self.batch_returns)\n",
    "                batch_log_probs = torch.stack(self.batch_log_probs)\n",
    "\n",
    "                loss = -(batch_returns * batch_log_probs).mean()\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "\n",
    "                self.batch_returns = []\n",
    "                self.batch_log_probs = []\n",
    "            \n",
    "            self.current_episode = []    \n",
    "\n",
    "    def get_action(self, state, epsilon=None):\n",
    "        \n",
    "        state_tensor = torch.tensor(state, device=self.device).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            logits:torch.FloatTensor = self.policy_net(state_tensor)\n",
    "            probabilities = logits.softmax(dim=-1)\n",
    "            action = torch.multinomial(probabilities, num_samples=1).item()\n",
    "            return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward before training =  8.492466843469796\n",
      "episode = 50 , reward =  47.01098220006083\n",
      "episode = 100 , reward =  47.716197667359054\n",
      "episode = 150 , reward =  49.789114336492894\n",
      "episode = 200 , reward =  52.68807266982624\n",
      "episode = 250 , reward =  46.254739336492904\n",
      "episode = 300 , reward =  42.31723933616742\n",
      "episode = 350 , reward =  43.75473933649288\n",
      "episode = 400 , reward =  40.28598933649289\n",
      "episode = 450 , reward =  42.31723933649281\n",
      "episode = 500 , reward =  42.50473933649289\n",
      "episode = 550 , reward =  48.1297393364929\n",
      "episode = 600 , reward =  44.379739336492904\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"highway-fast-v0\", render_mode=\"rgb_array\", config = config_dict)\n",
    "env.reset()\n",
    "\n",
    "action_space = env.action_space\n",
    "observation_space = env.observation_space\n",
    "\n",
    "gamma = .95\n",
    "episode_batch_size = 64\n",
    "learning_rate = 1e-2\n",
    "weight_decay = 1e-4\n",
    "model=CNN()\n",
    "optimizer = torch.optim.AdamW\n",
    "N_episodes = 1000\n",
    "\n",
    "agent = DQNAgent(\n",
    "    action_space=action_space,\n",
    "    observation_space=observation_space,\n",
    "    gamma=gamma,\n",
    "    episode_batch_size=episode_batch_size,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    ")\n",
    "\n",
    "print(\"mean reward before training = \", np.mean(eval_agent(agent, env, 10)))\n",
    "# Run the training loop\n",
    "train(env, agent, N_episodes, eval_every=50,)\n",
    "\n",
    "# Evaluate the final policy\n",
    "print(\"mean reward after training = \", np.mean(eval_agent(agent, env, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_one_episode(env, agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'get_image'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m100\u001b[39m):\n\u001b[32m     40\u001b[39m     action = env.action_space.sample()\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m     obs, reward, done, truncated, info = \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m     env.render()\n\u001b[32m     44\u001b[39m plt.imshow(env.render())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CS_cours/rl-2024--2025/.venv/lib/python3.12/site-packages/gymnasium/wrappers/common.py:393\u001b[39m, in \u001b[36mOrderEnforcing.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    391\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._has_reset:\n\u001b[32m    392\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[33m\"\u001b[39m\u001b[33mCannot call env.step() before calling env.reset()\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m393\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CS_cours/rl-2024--2025/.venv/lib/python3.12/site-packages/gymnasium/core.py:327\u001b[39m, in \u001b[36mWrapper.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    323\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\n\u001b[32m    324\u001b[39m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[32m    325\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[32m    326\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CS_cours/rl-2024--2025/.venv/lib/python3.12/site-packages/gymnasium/wrappers/common.py:285\u001b[39m, in \u001b[36mPassiveEnvChecker.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    283\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m.env, action)\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CS_cours/rl-2024--2025/.venv/lib/python3.12/site-packages/highway_env/envs/common/abstract.py:240\u001b[39m, in \u001b[36mAbstractEnv.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    235\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[32m    236\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe road and vehicle must be initialized in the environment implementation\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    237\u001b[39m     )\n\u001b[32m    239\u001b[39m \u001b[38;5;28mself\u001b[39m.time += \u001b[32m1\u001b[39m / \u001b[38;5;28mself\u001b[39m.config[\u001b[33m\"\u001b[39m\u001b[33mpolicy_frequency\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_simulate\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    242\u001b[39m obs = \u001b[38;5;28mself\u001b[39m.observation_type.observe()\n\u001b[32m    243\u001b[39m reward = \u001b[38;5;28mself\u001b[39m._reward(action)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CS_cours/rl-2024--2025/.venv/lib/python3.12/site-packages/highway_env/envs/common/abstract.py:280\u001b[39m, in \u001b[36mAbstractEnv._simulate\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    275\u001b[39m     \u001b[38;5;66;03m# Automatically render intermediate simulation steps if a viewer has been launched\u001b[39;00m\n\u001b[32m    276\u001b[39m     \u001b[38;5;66;03m# Ignored if the rendering is done offscreen\u001b[39;00m\n\u001b[32m    277\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    278\u001b[39m         frame < frames - \u001b[32m1\u001b[39m\n\u001b[32m    279\u001b[39m     ):  \u001b[38;5;66;03m# Last frame will be rendered through env.render() as usual\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m280\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_automatic_rendering\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    282\u001b[39m \u001b[38;5;28mself\u001b[39m.enable_auto_render = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CS_cours/rl-2024--2025/.venv/lib/python3.12/site-packages/highway_env/envs/common/abstract.py:340\u001b[39m, in \u001b[36mAbstractEnv._automatic_rendering\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    338\u001b[39m     \u001b[38;5;28mself\u001b[39m._record_video_wrapper.video_recorder.capture_frame()\n\u001b[32m    339\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m340\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CS_cours/rl-2024--2025/.venv/lib/python3.12/site-packages/highway_env/envs/common/abstract.py:308\u001b[39m, in \u001b[36mAbstractEnv.render\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    306\u001b[39m     \u001b[38;5;28mself\u001b[39m.viewer.handle_events()\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.render_mode == \u001b[33m\"\u001b[39m\u001b[33mrgb_array\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m308\u001b[39m     image = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mviewer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_image\u001b[49m()\n\u001b[32m    309\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m image\n",
      "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'get_image'"
     ]
    }
   ],
   "source": [
    "racing_config = {\n",
    "    'action': {\n",
    "        'lateral': True,\n",
    "        'longitudinal': True,\n",
    "        'target_speeds': [0, 30, 50, 80],\n",
    "        'type': 'ContinuousAction'\n",
    "    },\n",
    "    'action_reward': -0.1,\n",
    "    'collision_reward': -5,\n",
    "    'controlled_vehicles': 1,\n",
    "    'duration': 1200,\n",
    "    'lane_centering_cost': 1,\n",
    "    'lane_centering_reward': 0.1,\n",
    "    'manual_control': False,\n",
    "    'observation': {\n",
    "        'align_to_vehicle_axes': True,\n",
    "        'as_image': False,\n",
    "        'features': ['presence', 'velocity', 'acceleration'],\n",
    "        'grid_size': [[-30, 30], [-30, 30]],\n",
    "        'grid_step': [5, 5],\n",
    "        'type': 'OccupancyGrid'\n",
    "    },\n",
    "    'offscreen_rendering': False,\n",
    "    'other_vehicles': 3, \n",
    "    'other_vehicles_type': 'highway_env.vehicle.behavior.AggressiveVehicle',\n",
    "    'policy_frequency': 5,\n",
    "    'real_time_rendering': True,\n",
    "    'render_agent': True,\n",
    "    'scaling': 6,\n",
    "    'screen_height': 800,\n",
    "    'screen_width': 1200,\n",
    "    'show_trajectories': False,\n",
    "    'simulation_frequency': 15,\n",
    "}\n",
    "\n",
    "env = gym.make(\"racetrack-v0\", render_mode=\"rgb_array\", config=racing_config)\n",
    "env.reset()\n",
    "\n",
    "for _ in range(100):\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "    env.render()\n",
    "\n",
    "plt.imshow(env.render())\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'get_image'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 40\u001b[39m\n\u001b[32m     38\u001b[39m     action = env.action_space.sample()\n\u001b[32m     39\u001b[39m     obs, reward, done, truncated, info = env.step(action)\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m plt.imshow(env.render())\n\u001b[32m     43\u001b[39m env.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CS_cours/rl-2024--2025/.venv/lib/python3.12/site-packages/gymnasium/wrappers/common.py:409\u001b[39m, in \u001b[36mOrderEnforcing.render\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    404\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._disable_render_order_enforcing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._has_reset:\n\u001b[32m    405\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\n\u001b[32m    406\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCannot call `env.render()` before calling `env.reset()`, if this is an intended action, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    407\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mset `disable_render_order_enforcing=True` on the OrderEnforcer wrapper.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    408\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m409\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CS_cours/rl-2024--2025/.venv/lib/python3.12/site-packages/gymnasium/core.py:337\u001b[39m, in \u001b[36mWrapper.render\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    335\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> RenderFrame | \u001b[38;5;28mlist\u001b[39m[RenderFrame] | \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    336\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Uses the :meth:`render` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m337\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CS_cours/rl-2024--2025/.venv/lib/python3.12/site-packages/gymnasium/wrappers/common.py:303\u001b[39m, in \u001b[36mPassiveEnvChecker.render\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    301\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m env_render_passive_checker(\u001b[38;5;28mself\u001b[39m.env)\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m303\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CS_cours/rl-2024--2025/.venv/lib/python3.12/site-packages/highway_env/envs/common/abstract.py:308\u001b[39m, in \u001b[36mAbstractEnv.render\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    306\u001b[39m     \u001b[38;5;28mself\u001b[39m.viewer.handle_events()\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.render_mode == \u001b[33m\"\u001b[39m\u001b[33mrgb_array\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m308\u001b[39m     image = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mviewer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_image\u001b[49m()\n\u001b[32m    309\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m image\n",
      "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'get_image'"
     ]
    }
   ],
   "source": [
    "stablebaselines_config = {'action': {'type': 'DiscreteMetaAction'},\n",
    "'centering_position': [0.3, 0.5],\n",
    "'collision_reward': -1.0,\n",
    "'controlled_vehicles': 1,\n",
    "'duration': 45,\n",
    "'ego_spacing': 2,\n",
    "'high_speed_reward': 1.0,\n",
    "'initial_lane_id': None,\n",
    "'lane_change_reward': 0.2,\n",
    "'lanes_count': 4,\n",
    "'manual_control': False,\n",
    "'normalize_reward': True,\n",
    "'observation': {'type': 'Kinematics'},\n",
    "'offroad_terminal': False,\n",
    "'offscreen_rendering': False,\n",
    "'other_vehicles_type': 'highway_env.vehicle.behavior.AggressiveVehicle',\n",
    "'policy_frequency': 1,\n",
    "'real_time_rendering': False,\n",
    "'render_agent': True,\n",
    "'reward_speed_range': [30, 80],\n",
    "'right_lane_reward': -0.1,\n",
    "'scaling': 6.0,\n",
    "'screen_height': 300,\n",
    "'screen_width': 1200,\n",
    "'show_trajectories': False,\n",
    "'simulation_frequency': 15,\n",
    "'vehicles_count': 100,\n",
    "'vehicles_density': 2}\n",
    "\n",
    "env = gym.make(\"highway-fast-v0\", render_mode=\"rgb_array\", config=stablebaselines_config)\n",
    "env.reset()\n",
    "\n",
    "for _ in range(100):\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "    env.render()\n",
    "\n",
    "plt.imshow(env.render())\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
